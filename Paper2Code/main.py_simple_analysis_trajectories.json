[{"role": "system", "content": "You are an expert researcher, strategic analyzer and software engineer with a deep understanding of experimental design and reproducibility in scientific research.\nYou will receive a research paper in JSON format, an overview of the plan, a design in JSON format consisting of \"Implementation approach\", \"File list\", \"Data structures and interfaces\", and \"Program call flow\", followed by a task in JSON format that includes \"Required packages\", \"Required other language third-party packages\", \"Logic Analysis\", and \"Task list\", along with a configuration file named \"config.yaml\". \n\nYour task is to conduct a comprehensive logic analysis to accurately reproduce the experiments and methodologies described in the research paper. \nThis analysis must align precisely with the paper\u2019s methodology, experimental setup, and evaluation criteria.\n\n1. Align with the Paper: Your analysis must strictly follow the methods, datasets, model configurations, hyperparameters, and experimental setups described in the paper.\n2. Be Clear and Structured: Present your analysis in a logical, well-organized, and actionable format that is easy to follow and implement.\n3. Prioritize Efficiency: Optimize the analysis for clarity and practical implementation while ensuring fidelity to the original experiments.\n4. Follow design: YOU MUST FOLLOW \"Data structures and interfaces\". DONT CHANGE ANY DESIGN. Do not use public member functions that do not exist in your design.\n5. REFER TO CONFIGURATION: Always reference settings from the config.yaml file. Do not invent or assume any values\u2014only use configurations explicitly provided.\n     \n"}, {"role": "user", "content": "## Paper\n{'paper_id': '2504', 'title': 'Paper2Code: Automating Code Generation from Scientific Papers in Machine Learning', 'abstract': 'Despite the rapid growth of machine learning research, corresponding code implementations are often unavailable, making it slow and labor-intensive for researchers to reproduce results and build upon prior work. In the meantime, recent Large Language Models (LLMs) excel at understanding scientific documents and generating high-quality code. Inspired by this, we introduce PaperCoder, a multi-agent LLM framework that transforms machine learning papers into functional code repositories. PaperCoder operates in three stages: planning, where it constructs a high-level roadmap, designs the system architecture with diagrams, identifies file dependencies, and generates configuration files; analysis, which focuses on interpreting implementation-specific details; and generation, where modular, dependency-aware code is produced. Moreover, each phase is instantiated through a set of specialized agents designed to collaborate effectively across the pipeline. We then evaluate Pa-perCoder on generating code implementations from machine learning papers based on both model-based and human evaluations, specifically from the original paper authors, with author-released repositories as ground truth if available. Our results demonstrate the effectiveness of PaperCoder in creating high-quality, faithful implementations. Furthermore, it consistently shows strengths in the recently released PaperBench benchmark, surpassing strong baselines by substantial margins.', 'pdf_parse': {'paper_id': '2504', 'abstract': [{'text': 'Despite the rapid growth of machine learning research, corresponding code implementations are often unavailable, making it slow and labor-intensive for researchers to reproduce results and build upon prior work. In the meantime, recent Large Language Models (LLMs) excel at understanding scientific documents and generating high-quality code. Inspired by this, we introduce PaperCoder, a multi-agent LLM framework that transforms machine learning papers into functional code repositories. PaperCoder operates in three stages: planning, where it constructs a high-level roadmap, designs the system architecture with diagrams, identifies file dependencies, and generates configuration files; analysis, which focuses on interpreting implementation-specific details; and generation, where modular, dependency-aware code is produced. Moreover, each phase is instantiated through a set of specialized agents designed to collaborate effectively across the pipeline. We then evaluate Pa-perCoder on generating code implementations from machine learning papers based on both model-based and human evaluations, specifically from the original paper authors, with author-released repositories as ground truth if available. Our results demonstrate the effectiveness of PaperCoder in creating high-quality, faithful implementations. Furthermore, it consistently shows strengths in the recently released PaperBench benchmark, surpassing strong baselines by substantial margins.', 'section': 'Abstract', 'sec_num': None}], 'body_text': [{'text': 'Reproducibility lies at the heart of scientific progress, which enables researchers to validate findings, build upon prior work, and ultimately push the boundaries of knowledge [7, 3, 32] . However, across many disciplines, reproducing scientific results remains an enduring challenge. This is often due to incomplete documentation, missing experimental details, lack of access to data or proprietary tools, and, especially in machine learning research, the absence of corresponding code: for example, only 21.23 % of the papers accepted to top-tier machine learning conferences in 2024 provide their code implementations shown in Figure 1 . As a result, researchers frequently invest substantial effort in reverse-engineering methods and experimental results from papers, a process that is both timeconsuming and labor-intensive, subsequently slowing down the overall pace of scientific innovation.', 'section': 'Introduction', 'sec_num': '1'}, {'text': 'Meanwhile, recent Large Language Models (LLMs) have shown outstanding capabilities in understanding and generating both natural language and programming code [12, 30, 37] , with performances increasingly approaching or even surpassing that of domain experts in some scenarios. In addition, this progress has sparked growing interest in leveraging LLMs to accelerate scientific workflows, particularly in the early stages of ideation for new and valid research hypotheses [26, 46, 38, 2, 20, 47, 39] . Furthermore, some of these studies, as well as others focusing on later stages of automating experimental validations and improvements [16, 49, 4, 42] , demonstrate the potential of LLMs to generate code and even carry out experiments end-to-end; however, they typically assume and heavily rely on access to pre-existing implementations, partial code snippets, or well-defined APIs. As such, it remains questionable whether generating complete and faithful implementations solely from research papers (without access to prior code, APIs, or additional supplementary materials) can be achievable.', 'section': 'Introduction', 'sec_num': '1'}, {'text': 'To answer this question, we introduce PaperCoder, a multi-agent LLM-powered framework, designed to automatically generate executable code repositories in machine learning directly from and contextualized with research papers, which differs from prior work that requires partial implementations from human inputs. Specifically, PaperCoder aims to emulate the typical life cycle of human developers and researchers in writing the repository-level code, by decomposing the task into three structured stages: planning, analysis, and generation. First, during the planning stage, the proposed framework constructs a high-level roadmap to identify core components to implement, draws the overall system architecture with class and sequence diagrams to model structural relationships between modules, identifies file dependencies with their execution orders to guide correct build and execution flows, and generates configuration files to enable flexible customization of experimental workflows by human researchers. This is followed by the analysis stage, performing a fine-grained interpretation of each file and function with respect to their intended functionality, such as required inputs and outputs, interactions with other modules, and any algorithmic or architectural constraints derived from the source paper. Finally, in the generation stage, the framework synthesizes the entire code base based on the execution order determined earlier, along with the artifacts produced in the previous stages.', 'section': 'Introduction', 'sec_num': '1'}, {'text': 'To validate the effectiveness of PaperCoder, we conduct extensive evaluations on a subset of recent machine learning papers accepted at top-tier venues in 2024, including NeurIPS, ICML, and ICLR referred to as our proposed Paper2Code benchmark. Furthermore, we also incorporate the recently released PaperBench benchmark in our evaluation suite, enabling the fine-grained evaluations of code implementations with some papers from ICML 2024 [40] . Then, on a battery of tests conducted with automated model-based evaluations (covering both reference-free and reference-based settings, conditional on the availability of author-released ground-truth repositories) as well as expert human evaluations (based on authors of original papers), PaperCoder demonstrates substantial improvements over baselines, generating more valid and faithful code bases that could meaningfully support human researchers in understanding and reproducing prior work. Specifically, 77% of the generated repositories by PaperCoder are rated as the best, and 85% of human judges report that the generated repositories are indeed helpful. Also, further analyses show that each component of PaperCoder (consisting of planning, analysis, and generation) contributes to the performance gains, but also that the generated code bases can be executed, sometimes with only minor modifications (averaging 0.48% of total code lines) in cases where execution errors occur.', 'section': 'Introduction', 'sec_num': '1'}, {'text': 'short code snippets to solve isolated tasks, such as programming competition problems or simple coding queries, and was the primary focus in the early stages of LLM-based coding research [21, 6, 1, 14] . As LLMs have advanced in code comprehension, long-context reasoning, and handling complex workflows, research has increasingly shifted toward repository-level coding, where multiple files are generated while jointly considering architectural and functional requirements [23, 18, 25] .', 'section': 'Introduction', 'sec_num': '1'}, {'text': 'Several recent studies have explored this direction [48, 31] . Multi-agent frameworks such as ChatDev leverage LLMs to collaborate through structured dialogues [36] , while MetaGPT adopts a waterfall development model with role-specific generation stages [15] . These advancements highlight the growing capability of LLMs to support end-to-end software engineering tasks beyond single-file generation.', 'section': 'Introduction', 'sec_num': '1'}, {'text': 'LLM-Powered Research Scientific progress typically involves a cycle of idea generation, hypothesis validation, and experimental execution [33] . LLMs have been applied to various stages of this cycle, including research ideation [2, 20] , hypothesis generation [47, 35] , and peer review [8, 22, 44] , thereby helping researchers overcome existing limitations and accelerate scientific discovery [19, 26, 46] . In computer science and machine learning, where code-based experimentation is fundamental, LLMs have also been utilized to design experiments that enhance existing codebases.', 'section': 'Introduction', 'sec_num': '1'}, {'text': \"Many recent studies focus on refining implementations or optimizing performance, assuming the availability of the original implementation [16, 49, 4, 42] . However, this assumption limits their applicability in cases where source code is unavailable. Reproducibility plays a critical role in scientific discovery, enabling researchers to identify limitations, refine methodologies, and build upon prior work [32] . To evaluate LLMs' replication capabilities, PaperBench introduced a benchmark in which AI agents attempt to reproduce machine learning papers [40] . While this benchmark emphasizes evaluation, our work addresses a challenge of automatically generating code repositories from scientific papers in the absence of original implementations. To this end, we propose a framework that leverages LLMs for end-to-end research reproducibility. By focusing on repository-level reproduction, our approach expands the scope of LLM-powered automation beyond ideation and hypothesis generation, contributing to a critical yet underexplored aspect of scientific research.\", 'section': 'Introduction', 'sec_num': '1'}, {'text': 'Research Repository Generation In machine learning research, experiments are typically conducted using a code. However, in many cases, researchers do not release their code, making it challenging for others to reproduce and verify the proposed method and experiments [28, 32] . When non-authors attempt to re-implement the code manually, the process is often labor-intensive, timeconsuming. To address these challenges, we define the task of Research Repository Generation. The goal of this task is to read a paper and generate a corresponding repository that implements its methods and experiments. We define a repository as a collection of one or more code files necessary for reproducing the methods and experiments described in the paper. By automating this process, research repository generation facilitates the validation and reproducibility of scientific findings.', 'section': 'Problem Definition', 'sec_num': '3.1'}, {'text': \"Additionally, new research ideas often build upon prior work. However, when an researcher does not release their code, experimental validation becomes difficult, potentially hindering further advancements. Our approach aims to bridge this gap by enabling researchers to experimentally verify prior work, even when the original implementation is unavailable, thus contributing to a more reproducible and transparent research. In this study, we frame repository generation as a software development problem and assume that a model is responsible for writing the code to create a functional software repository. Given a research paper as input, the model generates a repository containing the code necessary to replicate the paper's methods and experiments. Formally, we define this as:\", 'section': 'Problem Definition', 'sec_num': '3.1'}, {'text': 'M (R) = C;', 'section': 'Problem Definition', 'sec_num': '3.1'}, {'text': 'where R represents the paper, C represents the generated code, and M is a function or model. While M can take various forms, large language models (LLMs) are a representative candidate. A language model functions as a mapping from an input x (text) to an output y (text or code), making it well-suited for this task.', 'section': 'Problem Definition', 'sec_num': '3.1'}, {'text': 'We introduce PaperCoder, a novel framework designed specifically for research repository generation that implements the methods and experiments described in a research paper, illustrated in Figure 2 . DatasetLoader class:...', 'section': 'Our Method', 'sec_num': '3.2'}, {'text': 'TransformerModel.. The goal of our work is to model the repository generation process as: M (R) = C where M is the model responsible for transforming the paper into an executable repository, R represents the research paper, and C represents the generated code. Inspired by software development methodologies, we adopt a structured approach that mirrors well-established, empirically validated software engineering principles. Specifically, we follow a plan-analysis-implementation workflow to systematically address the problem of research repository generation. To achieve this, we decompose the process into three sequential phases: 1) Planning. 2) Analysis. 3) Coding. Each phase utilizes task-specialized LLM agents that leveraging a multi-agent scenarios. Formally, we define the repository generation process as: C = M (P ) = M code (R, P, A); where R represents the research paper, P is the plan and A indicates the analysis. Each components are generated as follows: P = M plan (R), A = M analysis (R, P ) and C = M code (R, P, A). The following sections provide a detailed explanation of each phase.', 'section': 'model.py:', 'sec_num': '2.'}, {'text': \"Providing the research paper as a input to the model and expecting it to generate a complete repository is highly challenging. Research papers are primarily written to document discoveries and persuade readers, rather than to serve as structured input for software development. Consequently, papers often contain supplementary information that, while essential for conveying core concepts, is not directly relevant to implementation. It could be introduces noise, making repository generation difficult and less effective. To address this challenge, instead of solely using the paper as input, we propose decomposing the paper into a structured multi-aspect plan. This approach organizes the key implementation-relevant elements into four distinct components, ensuring that the generated repository is well-structured and aligned with the paper's methodology. Formally, M plan (R) = P = {o, d, l, g}; where o indicates the overall plan, d is the architecture design, l represents the logic design and g is the configuration file. Each output is generated in each phase, and this phase follows a sequential structure, where the output of each stage serves as the input for the next. Note that each phase utilized by the same agent which is the plan agent M plan .\", 'section': 'Planning', 'sec_num': '3.2.1'}, {'text': 'Overall Plan The first step within planning phase, the overall plan, involves summarizing and organizing the core elements necessary to implement the research repository from a high-level perspective. This summary provides an essential conceptual framework that guides subsequent steps clearly. Formally, M plan (R) = o, where R is the research paper and o indicates the overall plan.', 'section': 'Planning', 'sec_num': '3.2.1'}, {'text': \"The second stage involves architecting the software based on the overall plan generated in the previous stage and the research paper. Designing a well-structured architecture is essential, particularly for software systems where multiple features must interact seamlessly. This stage focuses on identifying the necessary components and defining their relationships to ensure a well-organized and functional repository. To achieve this, we asks to create key artifacts that define the software architecture. The file list is a structured collection of files required for the repository, outlining the software's modular structure. The class diagram provides a static representation of the system's data structures and interfaces. Using Unified Modeling Language (UML) notation, a standardized visual language for modeling software systems, we depict classes as rectangles, attributes and methods as lists, and relationships as connecting lines to illustrate how different components interact. The sequence diagram represents the program's call flow and object interactions dynamically, which uses UML notation to show actors as objects, messages as arrows, and lifelines as dashed lines, visually mapping out how components communicate over time. This structured approach ensures a clear and organized representation of the software's architecture. By constructing these artifacts, we visually represent the essential components described in the research paper, enabling a more structured and systematic approach to repository generation. This process facilitates better analysis of dependencies and relationships, ensuring that the generated repository aligns with the core idea of the paper. Formally, we define the architecture design process as: M plan (R, o) = d where R represents the paper, o refers to the previously generated overall plan, and d denotes the resulting architectural design artifacts.\", 'section': 'Architecture Design', 'sec_num': None}, {'text': \"Logic Design In software development, individual files rarely function in isolation. Instead, they generally exhibit interdependencies through imports and module interactions. For example, if a function A is defined in utils.py and later imported into evaluation.py, utils.py must be implemented before evaluation.py to maintain a correct dependency structure. To account for these dependencies, this stage takes as input the research paper along with the artifacts generated in the previous two stages. It then analyzes the logic of each file and its components, determining the necessary dependencies and the optimal execution order for implementation. As an output, it produces an ordered file list, detailing each file's role which files should be implemented considering dependencies and its dependencies within the repository. This approach ensures that repository generation considers not only individual file structures but also inter-file communication, facilitating a well-organized and logically coherent implementation. Formally, we define this dependency-aware file ordering process as M plan (R, o, d) = l where R represents the research paper, o refers to the overall plan, d denotes the architectural specifications, and l is the ordered file list which is the structured output defining the order and role of each file in the repository.\", 'section': 'Architecture Design', 'sec_num': None}, {'text': 'Configuration File Generation Finally, the configuration file generation step synthesizes all previously established outputs to produce a configuration file (config.yaml) containing hyperparameters and configuration required for model training. This process takes as input the overall plan, architectural design, and ordered file list produced in the previous stages. At this stage, users can review and modify the config.yaml file to identify and correct any missing or incorrectly specified details. For example, users may need to specify the path to a Hugging Face dataset or define the checkpoint storage directory. This step helps reduce hallucinations during the generation process, such as the model producing nonexistent datasets or referencing incorrect file paths. Formally, the configuration generation process is defined as M plan (R, o, d, l) = g where R represents the research paper, o refers to the overall plan, d denotes the software architecture specifications, l represents the logic design, and g is the resulting configuration file.', 'section': 'Architecture Design', 'sec_num': None}, {'text': 'While the planning phase primarily focuses on designing the overall repository structure and outlining a high-level roadmap, the analyzing phase delves into the implementation specifics of each individual file. In this phase, the detailed purpose and necessary considerations for each file within the repository are thoroughly analyzed. The outputs generated at this stage clearly specify what each file should achieve and highlight critical factors required for successful implementation. Specifically, the inputs to this analysis phase consist of the original research paper and previously generated artifacts (the overall plan, architecture design, logic designs, and configuration file). The outputs of this phase include file-level analyses documenting precise implementation details, which will later inform the code generation process. Formally, the file-level analysis process is defined as {M analysis (R,', 'section': 'Analyzing', 'sec_num': '3.2.2'}, {'text': 'P = {o, d, l, g}, f i ) = a i } n=|file| i=1', 'section': 'Analyzing', 'sec_num': '3.2.2'}, {'text': '; wheref \u2208 F, a \u2208 A where R represents the research paper, P includes four plans, which o refers to the overall plan, d denotes the architectural specifications, l represents the logic design which consists of structured dependencies among files, g is the configuration file, and f i specifies the target file for which the analysis is being conducted. This process is applied to each file individually to generate a a i which is the comprehensive file-level implementation plan.', 'section': 'Analyzing', 'sec_num': '3.2.2'}, {'text': 'The final stage of our methodology is the coding phase, which produces code constituting the research repository. The generation of each file is guided by the comprehensive outputs from previous phases: the research paper itself, the overall plan, architecture design, logic designs, the configuration file, file-specific analyses, and previously generated code. As repository files often exhibit import dependencies among themselves, we adhere strictly to the ordered file list established during the planning phase, ensuring sequential consistency. Formally, the file-specific code generation process is defined as', 'section': 'Coding', 'sec_num': '3.2.3'}, {'text': '{M coder (R, P = {o, d, l, g}, f i , a i ) = c i } n=|file| i=1', 'section': 'Coding', 'sec_num': '3.2.3'}, {'text': '; where f \u2208 F, a \u2208 A, c \u2208 C. This process is applied sequentially for each file to ensure a structured and logically coherent repository implementation c i . The initially produced code may require subsequent debugging or refinement to ensure correctness and full functionality. In this work, comprehensive debugging strategies and detailed error-correction workflows remain beyond the current scope of this paper.', 'section': 'Coding', 'sec_num': '3.2.3'}, {'text': 'Paper2Code Benchmark To evaluate the effectiveness of our framework for reproducing both the proposed methods and experiments described in scientific papers, we constructed a experimental benchmark based on papers from ICML 2024, NeurIPS 2024, and ICLR 2024. Using the OpenReview API 1 , we first filtered the accepted papers from each conference to include only those with publicly available GitHub repositories. Among these, we selected repositories with a total codebase of fewer than 70,000 tokens to ensure reproducibility within a manageable scope. To ensure quality, we performed model-based evaluations of the repositories and selected the top 30 highest-scoring papers from each venue. These 90 papers constitute our Paper2Code benchmark for experimental validation. A detailed list of the 90 papers is provided in Table 9 , Table 10 and Table 11 . For the human evaluation, we select 13 papers authored by the evaluation participants, and listed in Table 13 .', 'section': 'Data', 'sec_num': '4.1'}, {'text': 'PaperBench Code-Dev We further validate our framework using the PaperBench Code-Dev benchmark [40] , which consists of a curated set of 20 papers from ICML 2024.', 'section': 'Data', 'sec_num': '4.1'}, {'text': \"Paper2Code Benchmark To the best of our knowledge, there is no existing framework that directly addresses the task of end-to-end paper-to-code reproduction. However, we consider several related approaches as baselines, particularly those designed for software development from natural language inputs such as requirements. These serve as the closest comparison points in terms of producing functional code repositories. In addition, we include comparisons with a number of ablated variants that serve as simple or naive baselines, as follow: 1. ChatDev [36] -It is a multi-agent framework where LLM-driven agents collaboratively develop software through dialogues. In our experiments, the entire paper is provided as input(requirements) to generate a full code repository; 2. MetaGPT [15] -It adopts a role-based multi-agent paradigm, where software development is organized via Standardized Operating Procedures (SOPs), which specify structured workflows and responsibilities for each agent. We provide the entire paper as input and prompt the system to construct the full code repository; 3. Abstract -A naive baseline where only the paper's abstract is provided to the language model, requiring it to implement the code repository based on minimal information; 4. Paper -Another naive baseline in which the full paper is provided as input, and the model is prompted to generate a corresponding code repository; 5. PaperCoder (Ours) -Our proposed method, which integrates planning, analysis and code generation to generate high-quality repositories from scientific papers; 6. Oracle -The official code repository released by the paper's authors. This serves as an upper bound for comparison, reflecting the intended implementation of the proposed in the paper.\", 'section': 'Baselines and Our Model', 'sec_num': '4.2'}, {'text': 'PaperBench Code-Dev We compare our method against the two agent variants reported in the original benchmark. The Basic Agent follows a ReAct-style approach and is equipped with a predefined set of tools, including a bash shell command executor, a Python code runner, a web browser, and a paginated file reader for processing long documents. The Iterative Agent extends the Basic Agent by utilizing the full available computation time and incorporating custom prompting strategies that encourage step-by-step reasoning through subgoal decomposition (see Starace et al. [40] for details).', 'section': 'Baselines and Our Model', 'sec_num': '4.2'}, {'text': 'Evaluating code generation typically relies on unit tests or verifying specific implementation components [5, 11] . However, such evaluations are often infeasible due to the lack of gold-standard repositories. Many papers do not release official code, and even when they do, test scripts are frequently unavailable. Additionally, manually annotating each paper with ground-truth implementation details is highly labor-intensive, rendering large-scale evaluation impractical.', 'section': 'Paper2Code Benchmark', 'sec_num': '4.3.1'}, {'text': 'To address this challenge, we adopt a model-based evaluation approach [50, 13, 24] that assesses repository quality with or without access to gold-standard repository. We consider two variants: (1) reference-based, which leverages both the paper and the author-provided repository, and (2) reference-free, which evaluates the generated repository using only the paper. For both variants, we prompt the language model to critique the required implementation components and evaluate their correctness. It assigns severity levels (high, medium and low) to any missing or flawed elements and generates an overall correctness score on a 1 to 5 scale. To ensure stability, we report the average score across multiple generations using n-way sampling. In this experiments, we set the n = 8 and use the o3-mini-high as a evaluation model.', 'section': 'Model-Based Evaluation', 'sec_num': None}, {'text': 'When an official repository is available, we treat it as one plausible implementation of the method described in the paper. While multiple valid implementations may exist, the author-released version is generally considered the most accurate, as it best captures the core components necessary for reproducing experimental results. In this setting, we provide both the paper and the gold-standard repository as input, and prompt the evaluation model to identify and critique the required components. The model then compares the predicted repository against these components and assigns a correctness score from 1 to 5, reflecting both component coverage and the severity of any errors.', 'section': 'Reference-Based Evaluation', 'sec_num': None}, {'text': 'Reference-Free Evaluation In many cases, author-released code is unavailable. To handle such scenarios, we propose a reference-free evaluation strategy that relies solely on the paper and the generated repository. As in the reference-based setting, the model is prompted to infer the required implementation components from the paper, critique them, and assess whether they are adequately implemented in the generated repository. A correctness score from 1 to 5 is then assigned based on the presence and quality of these components.', 'section': 'Reference-Based Evaluation', 'sec_num': None}, {'text': \"Human Evaluation While model-based evaluation provides a scalable and automated approach of assessing performance, we additionally conduct human evaluation to offer a complementary and comparative assessment of our method against various baselines. Unlike model-based evaluation, which scores each repository independently, human annotators are instructed to comparatively rank repositories generated by different methods. Given the complexity of the task, which requires both understanding the paper and judging the faithfulness of its implementation, we recruit MS and PhD students majoring in computer science with experience authoring at least one peer-reviewed paper. To Table 1 : Results on our experimental Paper2Code benchmark. We report average model-based evaluation scores for each conference. Oracle denotes the official repository released by the paper's authors. Reference-based evaluation assesses correctness (on a 1-5 scale) by comparing the generated repository against both the paper and the official implementation, while reference-free evaluation relies solely on the paper. We report statistics on the average number of tokens, files, and functions per repository. The best scores are highlighted in bold. Rankings are converted into scores on a 1-5 scale. In groups with three candidates, the top-ranked repository receives 5 points, the second receives 3, and the third receives 1. After completing all group-level rankings, annotators are asked to select the overall best repository and provide a brief justification. They are also asked whether the top-ranked repository would make reproducing the paper's methods and experiments easier than starting from scratch. If not, they are required to explain why. Finally, annotators revisit the repository generated by our method and evaluate whether each of the previously defined implementation criteria is fully (\u20dd), partially (\u25b3) or not (\u00d7) satisfied. Detailed evaluation templates are provided in the Figure 11 .\", 'section': 'Reference-Based Evaluation', 'sec_num': None}, {'text': \"For evaluating our system on the PaperBench Code-Dev benchmark, we adopt the official evaluation protocol, which measures replication accuracy across a curated set of ICML 2024 papers. Specifically, we follow the rubric authored by the original paper's authors, which defines a hierarchical set of implementation requirements. The evaluation model assesses each generated repository based on whether the submitted code correctly implements any of the specified requirements. Importantly, the evaluation focuses only on the code development node which whether the candidate repository contains a correct implementation of some requirement.\", 'section': 'PaperBench Code-Dev Evaluation', 'sec_num': '4.3.2'}, {'text': 'Main Results Table 1 presents the main experimental results, our method consistently outperform across all conferences and both evaluation modes. Under the reference-based setting, our approach achieves the highest average correctness scores of 3.72, 3.83, and 3.68 on ICML, NeurIPS, and ICLR papers, respectively. Similarly, in the reference-free evaluation, our method attains scores of 4.73, 4.77, and 4.73, outperforming all baselines. Compared to software development work baselines such as ChatDev and MetaGPT, our method exhibits substantial performance gains. Notably, although ChatDev generates a comparable number of files (6.99 vs. 6.97), our method produces significantly more functions (35.22 vs. 23 .82), indicating a higher level of granularity and completeness in the generated repositories. In contrast, MetaGPT lags behind both in evaluation scores and in code quantity metrics. Ablation variants using only the abstract or full paper underperform relative to ours. This highlights the effectiveness of our multi-stage framework in systematically extracting and organizing implementation-relevant information. For reference, we include an Oracle score that represents the upper bound, corresponding to the official implementation by the authors. While there is still a gap to the Oracle, our method achieves comparable results that are closer to this upper bound compared to existing LLM-based frameworks. Overall, these results demonstrate that our proposed system not only surpasses prior approaches in generating accurate and comprehensive code repositories but also excels in producing fine-grained, detail-oriented implementations necessary for faithful reproduction of research methods. Correlation between Reference-based and Reference-free We investigate the alignment between reference-based and referencefree evaluation metrics by computing their correlation across a large set of generated repositories. As shown in Figure 3 , the two evaluation scores exhibit a strong positive correlation, with a Pearson correlation coefficient of r = 0.79 and a statistically significant p-value of 0.00. This high correlation suggests that reference-free evaluation which does not rely on access to ground-truth code can serve as a reliable proxy for reference-based assessment. The result supports the feasibility of using reference-free methods in scenarios where official implementations are unavailable, while still maintaining consistency with traditional evaluation standards. ', 'section': 'Experimental Results and Analysis', 'sec_num': '5'}, {'text': 'To assess the practical utility and subjective quality of the generated code, we conduct a comprehensive human evaluation. We include both model-based evaluations (reference-based and reference-free) and direct human assessments, where authors or expert annotators rank the quality of each generated repository. Table 3 summarizes the results across these three evaluation types. Across all evaluation criteria, our method consistently achieves the highest scores and the best rankings. In terms of average human evaluation scores, our system achieves 4.23 and 4.38 comparing both ablation variations and software development works, respectively, substantially outperforming all baselines. Similarly, in model-based reference-based and reference-free evaluations, our method attains 3.74 (ref-based) and 4.71 (ref-free), outperforming alternatives such as ChatDEV and MetaGPT. In the ranking-based evaluation, our method also secures the top position across all metrics. Notably, it achieves the best average rank in human evaluations (1.38 and 1.31), while other baselines such as MetaGPT and only Abstract consistently rank lower. The low variance in our ranking (e.g., 1.00 in both reference-based and reference-free) further demonstrates the robustness of our system across diverse evaluation settings. These results clearly indicate that our proposed approach produces more useful and accurate implementations than existing methods, regardless of whether the evaluation is automated or conducted by human experts.', 'section': 'Human Evaluation Results', 'sec_num': None}, {'text': \"We conduct a fine-grained human evaluation to better understand the practical utility and implementation quality of the generated repositories. Specifically, we asked authors to review a set of repositories about covering both baselines and ablated variants and to select the one they considered most suitable for reproducing their work. 77% (10 out of 13 authors) selected the repository generated by our method as the top choice, followed by 3 authors who preferred the Paper variant. The most common reasons cited for favoring our method include completeness, clean structure, and faithfulness to the original paper. Full results are shown in Table 8 . To further assess real-world usefulness, we asked participants whether the top-ranked repository would make it easier to reproduce the paper's methods and experiments compared to starting from scratch. As a result, 85% responded positively, reinforcing the practical value of our approach. In addition, we performed a section-level implementation analysis of our generated repositories. For each paper, we asked to define criteria per section-Data Processing, Method, and Evaluation-and asked authors to evaluate whether each criterion was fully implemented (\u20dd), partially implemented (\u25b3), or missing (\u00d7). These responses were numerically mapped to scores of 1, 0.5, and 0, respectively. The average number of criteria per section was 1.62 for Data Processing, 3.54 for Method, and 2.00 for Evaluation. As shown in Figure 4 , the coverage rates were 48%, 85%, and 70% for the three sections, respectively. Follow-up analysis of the \u25b3 and \u00d7 responses revealed that missing or incomplete implementations were the most common reasons for low scores in Data Processing and Evaluation, while Method sections were more often penalized for both missing logic and misunderstandings of the paper's core methodology. Overall, this section-level evaluation confirms that our method not only produces structurally sound code, but also provides substantial practical value as judged by original paper authors supporting both accurate implementation and reproducibility.\", 'section': 'Detailed Analysis on Generated Repository', 'sec_num': None}, {'text': 'Table 5 : Rank correlation between modelbased and human evaluations is measured using GPT-4o (gpt-4o-2024-11-20) and o3mini with high reasoning-effort.', 'section': 'Detailed Analysis on Generated Repository', 'sec_num': None}, {'text': 'Reference-based Reference-free GPT-4o o3-mini-high GPT-4o o3-mini-high 0.67 0.71 0.67 0.67 Analysis on Human Alignment for Evaluation We measure their correlation with human judgments for assessing the reliability of model-based evaluation metrics. Table 5 reports the rank correlation between human scores and model-based scores across both referencebased and reference-free settings, using two evaluation models: GPT-4o and o3-mini-high. The results show moderate to strong correlations in all settings. In the reference-based evaluation, o3-mini-high achieves the highest correlation with human judgments (0.67), followed closely by GPT-4o (0.63). In the reference-free setting, both models exhibit strong alignment with human scores, each achieving a correlation of 0.63. These results suggest that model-based evaluations can serve as reliable proxies for human judgment. This supports the use of automatic scoring in large-scale experiments and reinforces the credibility of the evaluation results presented in this study. Based on these findings, we adopt the o3-mini-high model as the evaluation model in all experiments.', 'section': 'Detailed Analysis on Generated Repository', 'sec_num': None}, {'text': 'Analysis on Different LLMs To investigate the impact of the underlying language model on overall performance, we experiment with four different LLM backbones: DS-Coder (DeepSeek-Coder-V2-Lite-Instruct), Qwen-Coder (Qwen2.5-Coder-7B-Instruct), DS-Distill-Qwen (DeepSeek-R1-Distill-Qwen-14B), and o3-mini-high (the high reasoning-effort variant of o3-mini). Each model is used within the our framework, and we evaluate the outputs using reference-based, reference-free, and human evaluation metrics. Results are summarized in Table 4 . We observe that o3-mini-high consistently outperforms all other models across all evaluation dimensions. In terms of referencebased and reference-free scores, it achieves 3.74 and 4.71, respectively, with a higher score of 4.79 in human evaluation. Notably, it positions the top ranking in all settings, with an average rank of 1.00 in both reference-based and reference-free metrics, and 1.15 in human evaluations. Among the other models, DS-Distill-Qwen performs best, followed by Qwen-Coder and DS-Coder. This performance trend holds consistently across both model-based scores and human judgments. These results highlight the importance of selecting a capable and well-aligned backbone model for generating high-quality, executable repositories from research papers. Based on this results, we mainly use the o3-mini-high as the basis backbone model for all experiments. Ablation Studies We conduct ablation experiments to examine the contribution of each module in our multistage pipeline. Starting with a base that uses only the full paper text (Paper), we incrementally introduce components in the order they are executed: overall plan, architecture design, core logic, configuration file, and finally the analysis module. As shown in Table 6 , performance steadily improves as additional steps are incorporated. Performance temporarily dips when the architecture design module is added. This can be attributed to the lack of dependency-aware ordering at this stage. For example, generating main.py before defining utils.py, which is only resolved later during the core logic phase. Once core logic is introduced, which defines file dependencies and generation order, scores improve substantially. Subsequent additions, such as the configuration file and final analysis stage, further refine the output. Our full system (Ours) achieves the highest scores under both reference-based (3.72) and reference-free (4.73) evaluation settings. These findings highlight the importance of sequential structuring in our pipeline.', 'section': 'Detailed Analysis on Generated Repository', 'sec_num': None}, {'text': 'To verify whether the generated code is not only structurally sound but also executable with minimal intervention, we conduct a manual debugging analysis on five representative papers. For each case, we attempts to execute the generated repository and records the number of lines modified to achieve a successful run. Table 7 describes the total number of lines, the modified lines required, and the percentage of changes per repository. Remarkably, we find that on average, only 0.48% of the code needs to be altered for successful execution. Most modifications involve routine fixes such as updating deprecated OpenAI API calls to their latest versions or correcting simple type conversions. These findings indicate that our system does not merely produce syntactically correct or superficially plausible code, but outputs repositories that are functionally robust and executable with minimal human correction.', 'section': 'Analysis on Executability', 'sec_num': None}, {'text': 'In this work, we introduce PaperCoder, the framework designed to automatically generate code repositories from machine learning research papers. The framework follows a three-stage pipeline planning, analysis, and coding. The planning phase includes the construction of an overall plan, architectural design via diagrams, core logic planning with file dependencies, and configuration file generation. This plan is then refined through a detailed analysis of per-file implementation requirements, followed by sequential code generation guided by the planning outputs.', 'section': 'Conclusion', 'sec_num': '6.1'}, {'text': \"We evaluate PaperCoder on a comprehensive experimental Paper2Code benchmark consisting of 90 recent papers from top-tier machine learning conferences, as well as the PaperBench Code-Dev benchmark. Our framework consistently outperforms existing baselines across model-based evaluation metrics. In addition, we conduct human evaluations with graduate-level computer science researchers, who assess the quality of repositories generated for their own papers. Results show that 77% of participants preferred PaperCoder's implementation over alternatives, and 83% found the outputs practically useful for real-world usage. These findings collectively demonstrate the effectiveness of PaperCoder in bridging the gap between research ideas and working code.\", 'section': 'Conclusion', 'sec_num': '6.1'}, {'text': \"While our PaperCoder shows strong performance on recent machine learning papers, its current scope is limited to this specific domain. Extending the framework to support a broader range of scientific fields remains an important direction for future work. Additionally, our primary evaluation relies on model-based metrics, which may not fully capture executability or runtime correctness. Although we perform case studies involving manual debugging to validate executability, a scalable and automated approach to execution-based evaluation-including fault localization and debugging-is a promising avenue for future research. Incorporating such capabilities would further enhance PaperCoder's utility in real-world research workflows. \", 'section': 'Limitations', 'sec_num': '6.2'}, {'text': 'A.', 'section': 'A Additional Experimental Designs', 'sec_num': None}, {'text': 'All experiments are conducted using the o3-mini with high reasoning effort version (o3-mini-high) as the default backbone, released on January 31, 2025. To collect paper metadata and content, we utilize the openreview_scraperfoot_1 and the Semantic Scholar API 3 . For document processing, we convert papers into structured JSON format using the s2orc-doc2json libraryfoot_3 .', 'section': 'Implementation Details', 'sec_num': '1'}, {'text': 'To conduct evaluations with human judges, we recruited 13 researchers from the South Korea, majoring in computer science, each with a minimum of 1 published papers. For annotation, they were provided with a 4-page document, which includes the task instructions, annotation examples, and 10 generated repositories grouped into three sets. Each repository was anonymized using a repoX naming format to prevent bias regarding the generation method. Following the question guidelines in the document, annotators reviewed and evaluated the repositories generated by different methods and models. We note that they were compensated at a rate of $X per hour. Also, on average, evaluating 10 repositories for a single paper took approximately 45 minutes. Table 11 is a detailed annotation example.', 'section': 'A.2 Human Evaluation Process', 'sec_num': None}, {'text': \"In the reference-based evaluation setting, the repository may exceed the model's context length. Following Starace et al. [40] , when this occurs, we prompt the model to select the most relevant files for evaluation. The selected subset is then used as the reference for scoring. We use the gpt-4o-2024-11-20 as the evaluation model.\", 'section': 'A.3 Reference-Based Evaluation', 'sec_num': None}, {'text': 'Prompt for generating the overall plan in the planning stage', 'section': 'A.3 Reference-Based Evaluation', 'sec_num': None}, {'text': '[System]', 'section': 'A.3 Reference-Based Evaluation', 'sec_num': None}, {'text': 'You are an expert researcher and strategic planner with a deep understanding of experimental design and reproducibility in scientific research.', 'section': 'A.3 Reference-Based Evaluation', 'sec_num': None}, {'text': \"You will receive a research paper in JSON format. Your task is to create a detailed and efficient plan to reproduce the experiments and methodologies described in the paper. This plan should align precisely with the paper's methodology, experimental setup, and evaluation metrics.\", 'section': 'A.3 Reference-Based Evaluation', 'sec_num': None}, {'text': 'Instructions:', 'section': 'A.3 Reference-Based Evaluation', 'sec_num': None}, {'text': '1. Align with the Paper: Your plan must strictly follow the methods, datasets, model configurations, hyperparameters, and experimental setups described in the paper.', 'section': 'A.3 Reference-Based Evaluation', 'sec_num': None}, {'text': '2. Be Clear and Structured: Present the plan in a well-organized and easy-to-follow format, breaking it down into actionable steps. 3. Prioritize Efficiency: Optimize the plan for clarity and practical implementation while ensuring fidelity to the original experiments.', 'section': 'A.3 Reference-Based Evaluation', 'sec_num': None}, {'text': '[User] ## Paper {paper_json} ## Task 1. We want to reproduce the method described in the attached paper.', 'section': 'A.3 Reference-Based Evaluation', 'sec_num': None}, {'text': '2. The authors did not release any official code, so we have to plan our own implementation.', 'section': 'A.3 Reference-Based Evaluation', 'sec_num': None}, {'text': '3. Before writing any Python code, please outline a comprehensive plan that covers:', 'section': 'A.3 Reference-Based Evaluation', 'sec_num': None}, {'text': \"-Key details from the paper's **Methodology**.\", 'section': 'A.3 Reference-Based Evaluation', 'sec_num': None}, {'text': '-Important aspects of **Experiments**, including dataset requirements, experimental settings, hyperparameters, or evaluation metrics.', 'section': 'A.3 Reference-Based Evaluation', 'sec_num': None}, {'text': '4. The plan should be as **detailed and informative** as possible to help us write the final code later.', 'section': 'A.3 Reference-Based Evaluation', 'sec_num': None}, {'text': \"## Requirements -You don't need to provide the actual code yet; focus on a **thorough, clear strategy**.\", 'section': 'A.3 Reference-Based Evaluation', 'sec_num': None}, {'text': '-If something is unclear from the paper, mention it explicitly.', 'section': 'A.3 Reference-Based Evaluation', 'sec_num': None}, {'text': \"The response should give us a strong roadmap, making it easier to write the code later. Your goal is to create a concise, usable, and complete software system design for reproducing the paper's method. Use appropriate open-source libraries and keep the overall architecture simple.\", 'section': '## Instruction', 'sec_num': None}, {'text': \"Based on the plan for reproducing the paper's main method, please design a concise, usable, and complete software system. Keep the architecture simple and make effective use of open-source libraries.\", 'section': '## Instruction', 'sec_num': None}, {'text': '--', 'section': '## Instruction', 'sec_num': None}, {'text': '## Format Example [CONTENT]', 'section': '## Instruction', 'sec_num': None}, {'text': '{ \"Implementation approach\": \"We will ... , \"File list\": [ \"main.py\", \"dataset_loader.py\", \"model.py\", \"trainer.py\", \"evaluation.py\" ], \"Data structures and interfaces\": \"Task list\": [ \"dataset_loader.py\", \"model.py\", \"trainer.py\", \"evaluation.py\", \"main.py\" ], \"Full API spec\": \"openapi: 3.0.0 ...\", \"Shared Knowledge\": \"Both data_preprocessing. Prompt for analyzing [System] You are an expert researcher, strategic analyzer and software engineer with a deep understanding of experimental design and reproducibility in scientific research. You will receive a research paper in JSON format, an overview of the plan, a design in JSON format consisting of \"Implementation approach\", \"File list\", \"Data structures and interfaces\", and \"Program call flow\", followed by a task in JSON format that includes \"Required packages\", \"Required other language third-party packages\", \"Logic Analysis\", and \"Task list\", along with a configuration file named \"config.yaml\".', 'section': '## Instruction', 'sec_num': None}, {'text': \"Your task is to conduct a comprehensive logic analysis to accurately reproduce the experiments and methodologies described in the research paper. This analysis must align precisely with the paper's methodology, experimental setup, and evaluation criteria.\", 'section': '## Instruction', 'sec_num': None}, {'text': '1. Align with the Paper: Your analysis must strictly follow the methods, datasets, model configurations, hyperparameters, and experimental setups described in the paper. 2. Be Clear and Structured: Present your analysis in a logical, well-organized, and actionable format that is easy to follow and implement. 3. Prioritize Efficiency: Optimize the analysis for clarity and practical implementation while ensuring fidelity to the original experiments. 4. Follow design: YOU MUST FOLLOW \"Data structures and interfaces\". DONT CHANGE ANY DESIGN. Do not use public member functions that do not exist in your design. 5. REFER TO CONFIGURATION: Always reference settings from the config.yaml file. Do not invent or assume any values-only use configurations explicitly provided. ', 'section': '## Instruction', 'sec_num': None}, {'text': \"Conduct a Logic Analysis to assist in writing the code, based on the paper, the plan, the design, the task and the previously specified configuration file (config.yaml). You DON'T need to provide the actual code yet; focus on a thorough, clear analysis.\", 'section': '## Instruction', 'sec_num': None}, {'text': 'Write the logic analysis in \\'{The name of the file to be generated\"}\\', which is intended for \\'{Description of the file generated through the \"Logic Analysis\" step of the logic design.}\\'.', 'section': '## Instruction', 'sec_num': None}, {'text': '--## Logic Analysis: {todo_file_name} Figure 9 : Prompts for analyzing. {} indicate placeholders to be filled with the content described in the accompanying explanation. The prompt is presented to the LLM for each file, following the sequence defined in the logic design.', 'section': '## Instruction', 'sec_num': None}, {'text': 'You are an expert researcher and software engineer with a deep understanding of experimental design and reproducibility in scientific research. You will receive a research paper in JSON format, an overview of the plan, a Design in JSON format consisting of \"Implementation approach\", \"File list\", \"Data structures and interfaces\", and \"Program call flow\", followed by a Task in JSON format that includes \"Required packages\", \"Required other language third-party packages\", \"Logic Analysis\", and \"Task list\", along with a configuration file named \"config.yaml\". Your task is to write code to reproduce the experiments and methodologies described in the paper.', 'section': 'Prompt for coding [System]', 'sec_num': None}, {'text': \"The code you write must be elegant, modular, and maintainable, adhering to Google-style guidelines. The code must strictly align with the paper's methodology, experimental setup, and evaluation metrics. Write code with triple quoto. \", 'section': 'Prompt for coding [System]', 'sec_num': None}, {'text': 'Based on the paper, plan, design, task and configuration file(config.yaml) specified previously, follow \"Format example\", write the code.', 'section': '# Instruction', 'sec_num': None}, {'text': 'We have {done_file_lst}. Next, you must write only the \"{todo_file_name}\". ', 'section': '# Instruction', 'sec_num': None}, {'text': 'https://docs.openreview.net/reference/api-v2', 'section': '', 'sec_num': None}, {'text': 'https://github.com/pranftw/openreview_scraper', 'section': '', 'sec_num': None}, {'text': 'https://www.semanticscholar.org/product/api', 'section': '', 'sec_num': None}, {'text': 'https://github.com/allenai/s2orc-doc2json', 'section': '', 'sec_num': None}], 'back_matter': [{'text': 'Aligning to thousands of preferences via system message generalization 3.5 For example, in Self-Instruct (TLDR; Self-Instruct, a framework for improving the instruction-following capabilities of language models by bootstrapping off their own generations.): 2. Given a set of repositories, which one is the most helpful for reproducibility-that is, which one best re-implements the methods and experiments as by the paper?Please review the provided repositories (Group 1: repo1-repo4, Group 2: repo5-repo7, Group 3: repo8-repo10) and rank them based on how well they are implemented.It is worth noting that the same repository may appear more than once between repo1 and repo10; this is not an error.(Optional things: Feel free to leave a comment explaining why you ranked them that way) ', 'section': 'annex', 'sec_num': None}], 'ref_entries': {'FIGREF0': {'uris': None, 'text': 'Figure 1: (a) PaperCoder overview. The proposed PaperCoder aims to transform given scientific papers (in machine learning domains) into code repositories, which consists of three sequential steps: planning, analyzing, and coding. (b) Code availability. The availability of the code repositories, where blue bars indicate the total number of accepted papers and orange regions represent the subset of papers with officially released code.', 'num': None, 'fig_num': '1', 'type_str': 'figure'}, 'FIGREF2': {'uris': None, 'text': \"Figure 2: PaperCoder framework. (Left) The naive approach, where a model directly generates code from the paper. (Right) PaperCoder framework, which decomposes the task into three stages: (1) Planning, where a high-level implementation plan is constructed based on the paper's content, including overall plan, architectural design, logic design, and configuration files; (2) Analyzing, where the plan is translated into detailed file-level specifications; and (3) Coding, where the final codes are generated to implement the paper's methods and experiments.\", 'num': None, 'fig_num': '32', 'type_str': 'figure'}, 'FIGREF4': {'uris': None, 'text': 'Figure 3: Correlation between reference-based and referencefree model-based evaluations.', 'num': None, 'fig_num': '3', 'type_str': 'figure'}, 'FIGREF5': {'uris': None, 'text': 'Figure 4: Human evaluation results on the helpfulness and section-level implementation accuracy of repositories generated by our framework.', 'num': None, 'fig_num': '4', 'type_str': 'figure'}, 'FIGREF6': {'uris': None, 'text': 'Figure 5: Prompt for generating the overall plan in the planning stage.', 'num': None, 'fig_num': '5', 'type_str': 'figure'}, 'FIGREF7': {'uris': None, 'text': 'flow\": \"\\\\nsequenceDiagram\\\\n participant M as Main\\\\n participant DL as DatasetLoader\\\\n participant MD as Model\\\\n participant TR as Trainer\\\\n participant EV as Evaluation\\\\n M-\u00bbDL: load_data()\\\\n DL-\u00bbM: return dataset\\\\n M-\u00bbMD: initialize model()\\\\n M-\u00bbTR: train(model, dataset)\\\\n TR-\u00bbMD: forward(x)\\\\n MD-\u00bbTR: predictions\\\\n TR-\u00bbM: training complete\\\\n M-\u00bbEV: evaluate(model, dataset)\\\\n EV-\u00bbMD: forward(x)\\\\n MD-\u00bbEV: predictions\\\\n EV-\u00bbM: metrics\\\\n\", \"Anything UNCLEAR\": \"Need clarification on the exact dataset format and any specialized hyperparameters.\" } [/CONTENT] ## Nodes: \"<node>: <type> # <instruction>\" -Implementation approach: <class \\'str\\'> # Summarize the chosen solution strategy. -File list: typing.List[str] # Only need relative paths. ALWAYS write a main.py or app.py here. -Data structures and interfaces: typing.Optional[str] # Use mermaid classDiagram code syntax, including classes, method(__init__ etc.) and functions with type annotations, CLEARLY MARK the RELATIONSHIPS between classes, and comply with PEP8 standards. The data structures SHOULD BE VERY DETAILED and the API should be comprehensive with a complete design. -Program call flow: typing.Optional[str] # Use sequenceDiagram code syntax, COMPLETE and VERY DETAILED, using CLASSES AND API DEFINED ABOVE accurately, covering the CRUD AND INIT of each object, SYNTAX MUST BE CORRECT. -Anything UNCLEAR: <class \\'str\\'> # Mention ambiguities and ask for clarifications. ## Constraint Format: output wrapped inside [CONTENT][/CONTENT] like the format example, nothing else.## Action Follow the instructions for the nodes, generate the output, and ensure it follows the format example.', 'num': None, 'fig_num': None, 'type_str': 'figure'}, 'FIGREF8': {'uris': None, 'text': 'Figure 6: Prompt for generating the architecture design in the planning stage. This prompt follows the previous prompt and response shown in Figure 5.', 'num': None, 'fig_num': '6', 'type_str': 'figure'}, 'FIGREF9': {'uris': None, 'text': 'py and trainer.py share ........\", \"Anything UNCLEAR\": \"Clarification needed on recommended hardware configuration for large-scale experiments.\" } [/CONTENT] ## Nodes: \"<node>: <type> # <instruction>\" -Required packages: typing.Optional[typing.List[str]] # Provide required third-party packages in requirements.txt format.(e.g., \\'numpy==1.21.0\\'). -Required Other language third-party packages: typing.List[str] # List down packages required for non-Python languages. If none, specify \"No third-party dependencies required\". -Logic Analysis: typing.List[typing.List[str]] # Provide a list of files with the classes/methods/functions to be implemented, including dependency analysis and imports. Include as much detailed description as possible. -Task list: typing.List[str] # Break down the tasks into a list of filenames, prioritized based on dependency order. The task list must include the previously generated file list. -Full API spec: <class \\'str\\'> # Describe all APIs using OpenAPI 3.0 spec that may be used by both frontend and backend. If front-end and back-end communication is not required, leave it blank. -Shared Knowledge: <class \\'str\\'> # Detail any shared knowledge, like common utility functions or configuration variables. -Anything UNCLEAR: <class \\'str\\'> # Mention any unresolved questions or clarifications needed from the paper or project scope. ## Constraint Format: output wrapped inside [CONTENT][/CONTENT] like the format example, nothing else.## Action Follow the node instructions above, generate your output accordingly, and ensure it follows the given format example.', 'num': None, 'fig_num': None, 'type_str': 'figure'}, 'FIGREF10': {'uris': None, 'text': 'Figure 7: Prompt for generating the logic design in the planning stage. This prompt follows the previous prompt and response shown in Figure 6.', 'num': None, 'fig_num': '7', 'type_str': 'figure'}, 'FIGREF11': {'uris': None, 'text': 'Prompt for generating the configuration file in the planning stage[User] You write elegant, modular, and maintainable code. Adhere to Google-style guidelines.Based on the paper, plan, design specified previously, follow the \"Format Example\" and generate the code. Extract the training details from the above paper (e.g., learning rate, batch size, epochs, etc.), follow the \"Format example\" and generate the code. DO NOT FABRICATE DETAILS -only use what the paper provides.You must write \\'config.yaml\\'. ATTENTION: Use \\'##\\' to SPLIT SECTIONS, not \\'#\\'. Your output format must follow the example below exactly.', 'num': None, 'fig_num': None, 'type_str': 'figure'}, 'FIGREF12': {'uris': None, 'text': 'Figure 8: Prompt for generating the configuration file in the planning stage. This prompt follows the previous prompt and response shown in Figure 7.', 'num': None, 'fig_num': '8', 'type_str': 'figure'}, 'FIGREF14': {'uris': None, 'text': 'Files {The content of the code files generated in the previous step.', 'num': None, 'fig_num': None, 'type_str': 'figure'}, 'FIGREF15': {'uris': None, 'text': 'Figure10: Prompts for coding. {} indicate placeholders to be filled with the content described in the accompanying explanation. The prompt is presented to the LLM for each file, following the sequence defined in the logic design. Previously generated code files are accumulated and provided as part of the ## Code Files input.', 'num': None, 'fig_num': '10', 'type_str': 'figure'}, 'TABREF0': {'content': '<table><tr><td/><td/><td>Reference-based</td><td/><td/><td>Reference-free</td><td/><td/><td>Statistics</td></tr><tr><td/><td>ICML</td><td>NeurIPS</td><td>ICLR</td><td>ICML</td><td>NeurIPS</td><td>ICLR</td><td colspan=\"2\"># of Tokens # of Files # of Funcs</td></tr><tr><td>ChatDEV</td><td colspan=\"6\">2.97 (0.58) 2.96 (0.69) 2.70(0.63) 4.12 (0.53) 4.01 (0.74) 4.00 (0.65)</td><td>6150.54</td><td>6.99</td><td>23.82</td></tr><tr><td>MetaGPT</td><td colspan=\"6\">2.75 (0.70) 2.95 (0.87) 2.48 (0.48) 3.63 (0.75) 3.59 (0.92) 3.52 (0.60)</td><td>5405.21</td><td>3.24</td><td>18.08</td></tr><tr><td>Abstract</td><td colspan=\"6\">2.43 (0.49) 2.35 (0.62) 2.28 (0.42) 3.01 (0.60) 2.99 (0.78) 3.03 (0.64)</td><td>3376.99</td><td>1.28</td><td>12.62</td></tr><tr><td>Paper</td><td colspan=\"6\">3.28 (0.67) 3.22 (0.80) 3.08 (0.66) 4.30 (0.53) 4.08 (0.84) 4.15 (0.63)</td><td>3846.33</td><td>1.79</td><td>14.84</td></tr><tr><td colspan=\"7\">PaperCoder 3.72 (0.54) 3.83 (0.50) 3.68 (0.52) 4.73 (0.44) 4.77 (0.38) 4.73 (0.32)</td><td>14343.38</td><td>6.97</td><td>35.22</td></tr><tr><td>Oracle</td><td>-</td><td>-</td><td>-</td><td colspan=\"3\">4.80 (0.32) 4.83 (0.38) 4.84 (0.26)</td><td>32149.04</td><td>28.00</td><td>122.03</td></tr><tr><td colspan=\"3\">ensure accurate judgment</td><td/><td/><td/><td/><td/></tr></table>', 'html': None, 'text': ', each participant is assigned a paper they have authored as the first author. The evaluation proceeds as follows. First, annotators define key implementation criteria based on the paper, covering the Data Processing, Method, and Evaluation sections. They then review and rank the generated repositories within the following three comparison groups: Group 1: Model Variants of Our Method. Repositories generated by our system using different backbone models (e.g., o3-mini vs. three open-source alternatives); Group 2: Naive Baselines. Repositories generated using only the Paper or the Abstract as input; Group 3: Related Works. Repositories generated by existing software development frameworks, such as MetaGPT and ChatDev. Within each group, annotators assign a relative ranking.', 'num': None, 'type_str': 'table'}, 'TABREF1': {'content': '<table><tr><td/><td/><td>Score (\u2191)</td><td/><td/><td>Ranking (\u2193)</td></tr><tr><td/><td>Ref-based</td><td>Ref-free</td><td>Human</td><td>Ref-based</td><td>Ref-free</td><td>Human</td></tr><tr><td>Abstract</td><td colspan=\"6\">2.36 (0.34) 2.99 (0.53) 1.62 (1.26) 3.00 (0.00) 3.00 (0.00) 2.69 (0.63)</td></tr><tr><td>Paper</td><td>3.19 (0.44)</td><td>4.22(0.52)</td><td colspan=\"4\">3.15 (1.28) 1.86 (0.36) 1.79 (0.43) 1.92 (0.64)</td></tr><tr><td colspan=\"2\">PaperCoder (Ours) 3.74 (0.30)</td><td colspan=\"5\">4.71 (0.27) 4.23 (1.30) 1.14 (0.36) 1.07 (0.27) 1.38 (0.65)</td></tr><tr><td>ChatDev</td><td>2.67(0.63)</td><td colspan=\"5\">3.87 (0.36) 2.69 (1.11) 2.50 (0.52) 2.36 (0.50) 2.15 (0.55)</td></tr><tr><td>MetaGPT</td><td colspan=\"6\">2.65 (0.46) 3.38 (0.66) 1.77 (1.30) 2.07 (0.52) 2.14 (0.52) 2.61 (0.65)</td></tr><tr><td>PaperCoder (</td><td/><td/><td/><td/><td/></tr></table>', 'html': None, 'text': 'Human evaluation results. For model-based evaluations, including reference-based and reference-free settings, correctness scores are converted into rankings for comparability. In the human evaluation, 13 authors were asked to rank repositories within two groups based on method type. Rankings are then converted into scores by assigning 5, 3, and 1 points to the first-, second-, and third-ranked repositories, respectively. Ours) 3.74 (0.30) 4.71 (0.27) 4.54 (1.20) 1.00 (0.00) 1.00 (0.00) 1.23 (0.60)', 'num': None, 'type_str': 'table'}, 'TABREF2': {'content': '<table><tr><td>Model</td><td>Replication scores (%)</td></tr><tr><td>BasicAgent</td><td>5.1 \u00b1 0.8</td></tr><tr><td>IterativeAgent</td><td>16.4 \u00b1 1.4</td></tr><tr><td>PaperCoder</td><td>44.26</td></tr></table>', 'html': None, 'text': 'Results on the PaperBench Code-Dev benchmark using o3-mini-high. For BasicAgent and IterativeAgent, results are averaged over three runs, and standard errors are reported.', 'num': None, 'type_str': 'table'}, 'TABREF3': {'content': '<table><tr><td/><td/><td>DS-Coder</td><td colspan=\"3\">Qwen-Coder DS-Distill-Qwen o3-mini-high</td></tr><tr><td>Score (\u2191)</td><td colspan=\"2\">Ref-based 1.63 (0.43)</td><td>1.8 (0.28)</td><td>2.07 (0.30)</td><td>3.74 (0.30)</td></tr><tr><td/><td>Ref-free</td><td>1.82 (0.39)</td><td>2.1 (0.28)</td><td>2.29 (0.29)</td><td>4.71 (0.27)</td></tr><tr><td/><td>Human</td><td>1.41 (0.64)</td><td>2.74 (1.14)</td><td>3.05 (1.04)</td><td>4.79 (0.74)</td></tr><tr><td colspan=\"3\">Ranking (\u2193) Ref-based 3.36 (0.93)</td><td>2.93 (0.62)</td><td>2.36 (0.63)</td><td>1.00 (0.00)</td></tr><tr><td/><td>Ref-free</td><td>3.36 (0.84)</td><td>2.86 (0.66)</td><td>2.14 (0.36)</td><td>1.00 (0.00)</td></tr><tr><td/><td>Human</td><td>3.69 (0.48)</td><td>2.69 (0.85)</td><td>2.46 (0.78)</td><td>1.15 (0.55)</td></tr></table>', 'html': None, 'text': 'Human evaluation results on model variants of PaperCoder. DS-Coder refers to DeepSeek-Coder-V2-Lite-Instruct, Qwen-Coder to Qwen2.5-Coder-7B-Instruct, and DS-Distil-Qwen to DeepSeek-R1-Distill-Qwen-14B. o3-mini-high denotes the high reasoning-effort of o3-mini. Other settings follow Table3.', 'num': None, 'type_str': 'table'}, 'TABREF4': {'content': '<table><tr><td colspan=\"3\">: Ablation study results on Paper-</td></tr><tr><td colspan=\"3\">Coder. We report the average scores and stan-</td></tr><tr><td colspan=\"3\">dard deviations on the ICML 2024 subset of</td></tr><tr><td colspan=\"3\">Paper2Code benchmark using model-based</td></tr><tr><td colspan=\"3\">reference-based and reference-free evalua-</td></tr><tr><td>tion.</td><td/></tr><tr><td/><td>Ref-based</td><td>Ref-free</td></tr><tr><td>Abstract</td><td colspan=\"2\">2.43 (0.49) 3.01 (0.60)</td></tr><tr><td>Paper</td><td colspan=\"2\">3.28 (0.67) 4.30 (0.53)</td></tr><tr><td>+ Overall Plan</td><td colspan=\"2\">3.40 (0.57) 4.34 (0.58)</td></tr><tr><td>+ Arch. Design</td><td colspan=\"2\">3.13 (0.68) 4.07 (0.74)</td></tr><tr><td>+ Logic Design</td><td colspan=\"2\">3.60 (0.52) 4.50 (0.57)</td></tr><tr><td>+ Config File</td><td colspan=\"2\">3.66 (0.45) 4.45 (0.53)</td></tr><tr><td colspan=\"3\">+ Analysis (Ours) 3.72 (0.54) 4.73 (0.44)</td></tr></table>', 'html': None, 'text': '', 'num': None, 'type_str': 'table'}, 'TABREF5': {'content': '<table><tr><td>Repo Name</td><td colspan=\"6\">CoLoR cognitive-behaviors RADA Self-Instruct G-EVAL Average</td></tr><tr><td>Modified lines</td><td>2</td><td>1</td><td>1</td><td>25</td><td>10</td><td>6.5</td></tr><tr><td>Total lines</td><td>1132</td><td>2060</td><td>1609</td><td>1334</td><td>1374</td><td>1251.5</td></tr><tr><td>Percentage</td><td>0.18</td><td>0.05</td><td>0.06</td><td>1.87</td><td>0.73</td><td>0.48</td></tr></table>', 'html': None, 'text': 'Executability results across generated repositories. We manually select five papers and generate corresponding repositories using PaperCoder. For each repository, we report the number of lines modified during debugging, the total number of lines, and the percentage of modified lines.', 'num': None, 'type_str': 'table'}, 'TABREF6': {'content': '<table><tr><td colspan=\"6\">Completeness Clean Structure Faithfulness to Paper Ease of Use Code Quality Unique Strengths</td></tr><tr><td>8</td><td>6</td><td>5</td><td>4</td><td>2</td><td>3</td></tr></table>', 'html': None, 'text': 'Qualitative analysis of top-ranked repositories. We categorize the reasons why human annotators selected the repositories generated by our framework as their top choice.', 'num': None, 'type_str': 'table'}, 'TABREF7': {'content': '<table><tr><td>--</td></tr><tr><td>## Format Example</td></tr><tr><td>[CONTENT]</td></tr><tr><td>{</td></tr><tr><td>\"Required packages\": [</td></tr><tr><td>\"numpy==1.21.0\",</td></tr><tr><td>\"torch==1.9.0\"</td></tr><tr><td>],</td></tr><tr><td>\"Required Other language third-party packages\": [</td></tr><tr><td>\"No third-party dependencies required\"</td></tr><tr><td>],</td></tr><tr><td>\"Logic Analysis\": [</td></tr><tr><td>[</td></tr><tr><td>\"data_preprocessing.py\",</td></tr><tr><td>\"DataPreprocessing class ........\"</td></tr><tr><td>],</td></tr><tr><td>[</td></tr><tr><td>\"trainer.py\",</td></tr><tr><td>\"Trainer ....... \"</td></tr><tr><td>],</td></tr><tr><td>[</td></tr><tr><td>\"dataset_loader.py\",</td></tr><tr><td>\"Handles loading and ........\"</td></tr><tr><td>],</td></tr><tr><td>[</td></tr><tr><td>\"model.py\",</td></tr><tr><td>\"Defines the model .......\"</td></tr><tr><td>],</td></tr><tr><td>[</td></tr><tr><td>\"evaluation.py\",</td></tr><tr><td>\"Evaluation class ........ \"</td></tr><tr><td>],</td></tr><tr><td>[</td></tr><tr><td>\"main.py\",</td></tr><tr><td>\"Entry point .......\"</td></tr><tr><td>]</td></tr><tr><td>],</td></tr></table>', 'html': None, 'text': \"Prompt for generating the logic design in the planning stage [User] Your goal is break down tasks according to PRD/technical design, generate a task list, and analyze task dependencies. You will break down tasks, analyze dependencies.You outline a clear PRD/technical design for reproducing the paper's method and experiments. Now, let's break down tasks according to PRD/technical design, generate a task list, and analyze task dependencies. The Logic Analysis should not only consider the dependencies between files but also provide detailed descriptions to assist in writing the code needed to reproduce the paper.\", 'num': None, 'type_str': 'table'}, 'TABREF8': {'content': '<table><tr><td>7. Write out EVERY CODE DETAIL, DON\\'T LEAVE TODO.</td></tr><tr><td>8. REFER TO CONFIGURATION: you must use configuration from \"config.yaml\". DO NOT FABRICATE any configuration</td></tr><tr><td>values.</td></tr><tr><td>{detailed_logic_analysis}</td></tr><tr><td>## Code: {todo_file_name}</td></tr></table>', 'html': None, 'text': '1. Only One file: do your best to implement THIS ONLY ONE FILE. 2. COMPLETE CODE: Your code will be part of the entire project, so please implement complete, reliable, reusable code snippets. 3. Set default value: If there is any setting, ALWAYS SET A DEFAULT VALUE, ALWAYS USE STRONG TYPE AND EXPLICIT VARIABLE. AVOID circular import. 4. Follow design: YOU MUST FOLLOW \"Data structures and interfaces\". DONT CHANGE ANY DESIGN. Do not use public member functions that do not exist in your design. 5. CAREFULLY CHECK THAT YOU DONT MISS ANY NECESSARY CLASS/FUNCTION IN THIS FILE. 6. Before using a external variable/module, make sure you import it first.', 'num': None, 'type_str': 'table'}, 'TABREF9': {'content': '<table><tr><td>Paper</td></tr></table>', 'html': None, 'text': 'List of ICLR 2024 papers used in our experimental benchmark. We evaluate each paper using the model-based, reference-free setting, with gpt-4o-2024-11-20 as the evaluation model.', 'num': None, 'type_str': 'table'}, 'TABREF10': {'content': '<table><tr><td>Paper</td></tr></table>', 'html': None, 'text': 'List of NeurIPS 2024 papers used in our experimental benchmark. We evaluate each paper using the model-based, reference-free setting, with gpt-4o-2024-11-20 as the evaluation model.', 'num': None, 'type_str': 'table'}, 'TABREF11': {'content': '<table><tr><td>Paper</td></tr></table>', 'html': None, 'text': 'List of ICML 2024 papers used in our experimental benchmark. We evaluate each paper using the model-based, reference-free setting, with gpt-4o-2024-11-20 as the evaluation model.', 'num': None, 'type_str': 'table'}}}}\n\n-----\n\n## Overview of the plan\nBelow is a detailed roadmap for reproducing the PaperCoder methodology and its experimental evaluation:\n\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n1. Overview and Objective  \n\u2003\u2022 Goal: Reproduce an end-to-end system (\u201cPaperCoder\u201d) that automatically generates functional code repositories from machine learning research papers.  \n\u2003\u2022 Key idea: Emulate human-like software development by decomposing the process into three sequential stages\u2014Planning, Analysis, and Code Generation\u2014using specialized LLM agents.  \n\u2003\u2022 Outcome: A system that, given a research paper (supplied in a structured JSON format), outputs a repository that implements the paper\u2019s methods and experiments.\n\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n2. Reconstructing the Pipeline (Methodology)  \nA. Planning Stage  \n\u20031. Overall Plan Generation  \n\u2003\u2003\u2013 Input: Full-text paper in JSON format.  \n\u2003\u2003\u2013 Task: Use an LLM agent to summarize and organize the core implementation elements (e.g., key methods, experimental procedures, and system goals).  \n\u2003\u2003\u2013 Output: A high-level \u201coverall plan\u201d document capturing essential components and guiding later stages.  \n\u2003\u2003\u2013 Note: If any aspect (e.g., unconventional method descriptions or ambiguous experimental details) is unclear, explicitly flag these for follow-up clarification.\n\n\u20032. Architectural Design  \n\u2003\u2003\u2013 Task: Translate the overall plan into a concrete system architecture by:\n\u2003\u2003\u2003\u2022 Generating a file list that outlines all necessary modules (e.g., main driver, dataset loader, model definition, training loop, evaluation script).  \n\u2003\u2003\u2003\u2022 Producing UML-style class diagrams (using tools like mermaid/PlantUML) to depict classes, attributes, methods, and relationships.  \n\u2003\u2003\u2003\u2022 Creating a sequence diagram to capture the inter-module call flow (e.g., how main.py invokes dataset loading, model initialization, training, and evaluation).\n\u2003\u2003\u2013 Output: A set of visual and textual artifacts that define file dependencies and execution order.\n\n\u20033. Logic (Core) Design  \n\u2003\u2003\u2013 Task: Refine the architectural design by determining a dependency-aware order for file generation.  \n\u2003\u2003\u2003\u2022 For each file, define its role, input\u2013output contracts, and necessary interactions with other modules.  \n\u2003\u2003\u2013 Output: An ordered file list with detailed logic specifications for each file (this ensures later code generation adheres to proper dependency order).\n\n\u20034. Configuration File Generation  \n\u2003\u2003\u2013 Task: Automatically produce a \u201cconfig.yaml\u201d that aggregates hyperparameters and settings needed for training and experimentation, such as:  \n\u2003\u2003\u2003\u2022 Dataset paths (or instructions to download and preprocess data).  \n\u2003\u2003\u2003\u2022 Model training parameters (learning rate, batch size, epochs) if applicable, or default values when details are not specified.  \n\u2003\u2003\u2003\u2022 Any other experimental settings (e.g., random seeds, evaluation metric thresholds).  \n\u2003\u2003\u2013 Output: A configuration file that serves as a backbone for reproducibility and minimizes hallucination (e.g., by preventing reference to non-existent datasets).\n\nB. Analysis Stage  \n\u2003\u2013 Task: For every file identified in the planning stage, carry out a detailed file-level analysis.  \n\u2003\u2003\u2022 Determine and document the specific functionality each file must implement, including its input parameters, outputs, and interdependencies.  \n\u2003\u2003\u2022 Ensure that algorithmic details (such as the logic for handling missing experimental details) are clearly outlined.  \n\u2003\u2013 Output: A comprehensive analysis document that bridges high-level planning to low-level code generation.\n\nC. Code Generation Stage  \n\u2003\u2013 Task: Sequentially generate code for each file following the dependency order from the logic design.  \n\u2003\u2003\u2022 Use outputs from the planning and analysis stages to drive a dependency-aware code generation process.  \n\u2003\u2003\u2022 Maintain modularity to allow individual files to be tested independently and ensure minimal post-generation debugging (as reported, only minor modifications\u2014around 0.48% of total lines\u2014are expected).  \n\u2003\u2013 Output: A fully structured code repository that, when assembled, faithfully reproduces the paper\u2019s experimental methods and results.\n\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n3. Experimental Setup  \nA. Data Collection and Processing  \n\u20031. Paper2Code Benchmark  \n\u2003\u2003\u2013 Assemble a dataset of ~90 machine learning papers (from ICML, NeurIPS, ICLR 2024) that meet the criteria (e.g., official repositories exist and total code size is <70K tokens).  \n\u2003\u2003\u2013 Use the OpenReview API to extract paper metadata and filters.  \n\u2003\u2003\u2013 Retrieve supplementary information via the Semantic Scholar API.  \n\u2003\u2003\u2013 Convert paper PDFs to structured JSON using the s2orc-doc2json tool.\n\n\u20032. PaperBench Code-Dev Benchmark  \n\u2003\u2003\u2013 Include the curated set of 20 ICML 2024 papers as an additional validation suite.\n\nB. LLM Backbone and System Configuration  \n\u2003\u2013 Primary LLM: Use the o3-mini-high (a high reasoning-effort variant) as the default backbone for all generation phases.  \n\u2003\u2013 Experiment with alternative LLM backbones (e.g., DS-Coder, Qwen-Coder, DS-Distill-Qwen) for ablation studies.  \n\u2003\u2013 Ensure that all agents (planning, analysis, and code generation) follow a sequential chain, reusing plans as input for subsequent stages.\n\nC. Baselines and Comparative Methods  \n\u2003\u2013 Implement baseline methods for comparison:  \n\u2003\u2003\u2022 ChatDev: A multi-agent framework using the full paper as \u201crequirements\u201d.  \n\u2003\u2003\u2022 MetaGPT: A role-based agent system with standard operating procedures for software development.  \n\u2003\u2003\u2022 Naive baselines: Single-stage approaches that either take only the abstract or the full paper without additional structured planning.  \n\u2003\u2013 Compare performance in terms of generated repository correctness, token count, number of files/functions, and procedural completeness.\n\nD. Experimental Variables and Hyperparameters  \n\u2003\u2013 Use n=8 (n-way sampling) to average model-based evaluation scores.  \n\u2003\u2013 Configure evaluation via:  \n\u2003\u2003\u2022 Reference-based metric: Compare generated repository to available author-released code (where possible) using an evaluation model (e.g., GPT-4o-2024-11-20).  \n\u2003\u2003\u2022 Reference-free metric: Evaluate correctness using only the paper and generated repository, also on a 1-to-5 scale.  \n\u2003\u2013 Specify default (or paper-suggested) hyperparameters in the \u201cconfig.yaml\u201d file.\n\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n4. Evaluation Metrics and Procedures  \nA. Model-Based Evaluation  \n\u2003\u2013 Reference-Based:  \n\u2003\u2003\u2022 When an official repository is available, use it as the gold standard.  \n\u2003\u2003\u2022 Prompt the evaluation model to compare required components and assign a correctness score (1\u20135).  \n\u2003\u2013 Reference-Free:  \n\u2003\u2003\u2022 Evaluate the generated code solely based on the paper\u2019s description.  \n\u2003\u2003\u2022 Use the same scoring approach to determine overall correctness.\n\nB. Human Evaluation  \n\u2003\u2013 Recruit approximately 13 expert evaluators (MS/PhD students with at least one published paper) to judge the outputs.  \n\u2003\u2013 Provide each evaluator with anonymized repositories (using a standardized naming scheme) along with instructions to:  \n\u2003\u2003\u2022 Rank the repositories based on correctness, completeness, clean structure, and faithfulness.  \n\u2003\u2003\u2022 Convert rankings to scores (e.g., first = 5 points, second = 3, third = 1).  \n\u2003\u2013 Capture qualitative feedback (e.g., which repository would ease reproduction compared to starting from scratch).\n\nC. Ablation and Comparative Studies  \n\u2003\u2013 Systematically disable or remove individual pipeline modules (e.g., architecture design, analysis) to gauge each component\u2019s impact on final performance.  \n\u2003\u2013 Examine metrics such as average correctness score (both reference-based and reference-free) and additional statistics (total number of tokens, file counts, function counts).\n\nD. Executability Checks  \n\u2003\u2013 Manually test a subset of generated repositories to ensure they run with minimal debugging (targeting modifications under 0.5% of code lines).  \n\u2003\u2013 Document any required fixes to validate robustness.\n\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n5. Infrastructure, Tools, and Timeline  \nA. Infrastructure and Tools  \n\u2003\u2013 Programming language: Python 3.x.  \n\u2003\u2013 Integration:  \n\u2003\u2003\u2022 OpenReview API for paper metadata.  \n\u2003\u2003\u2022 Semantic Scholar API for supplementary research details.  \n\u2003\u2003\u2022 s2orc-doc2json for PDF-to-JSON conversion.  \n\u2003\u2013 Additional libraries: Diagram generation tools (e.g., mermaid or PlantUML) for UML diagrams; LLM interfacing libraries (such as Hugging Face Transformers) as needed.  \n\u2003\u2013 Version control: Use Git to manage code repository iterations.\n\nB. Timeline and Execution Plan  \n\u20031. Data Collection and Initial Setup (\u22482 weeks)  \n\u2003\u2003\u2013 Implement API scrapers and PDF converters; assemble the benchmark datasets.  \n\u20032. Pipeline Implementation (\u22484\u20136 weeks)  \n\u2003\u2003\u2013 Develop and test the planning stage agents (overall plan, architectural and logic design, config generation).  \n\u2003\u2003\u2013 Build the analysis stage and integrate with code generation, ensuring dependency-aware ordering.  \n\u20033. Experimental Runs and Baseline Comparisons (\u22483\u20134 weeks)  \n\u2003\u2003\u2013 Execute the complete pipeline on the Paper2Code and PaperBench datasets.  \n\u2003\u2003\u2013 Run baseline models (ChatDev, MetaGPT, naive approaches) for side-by-side comparisons.  \n\u20034. Evaluation and Human Annotation (\u22482\u20133 weeks)  \n\u2003\u2003\u2013 Conduct model-based assessments using GPT-4o-2024-11-20 or o3-mini-high as the evaluation model.  \n\u2003\u2003\u2013 Coordinate with human evaluators to gather ranking data and qualitative feedback.  \n\u20035. Reporting and Iteration (\u22482 weeks)  \n\u2003\u2003\u2013 Analyze all experimental data, compile results, and document any encountered issues or necessary clarifications.\n\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n6. Uncertainties and Clarifications Needed  \n\u2003\u2022 LLM-specific settings: Precise configuration parameters or fine-tuning details for o3-mini-high and alternate backbones need clarification.  \n\u2003\u2022 Debugging Workflow: Although minimal debugging is reported, further details on automatic vs. manual error correction strategies would be useful.  \n\u2003\u2022 Hyperparameter Specification: Exact model training hyperparameters (e.g., learning rate, batch size, number of epochs) are not fully detailed in the paper and may require assumed defaults or follow-up clarification.  \n\u2003\u2022 File Dependency Details: While a dependency-aware file ordering is prescribed, some nuances (e.g., inter-module import specifics) may need manual adjustment during initial implementation.\n\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n7. Conclusion  \nThis roadmap lays out an end-to-end strategy to faithfully reproduce the PaperCoder system. It covers:  \n\u2003\u2013 A modular replication of the multi-agent pipeline (planning, analysis, code generation).  \n\u2003\u2013 Detailed experimental setup including dataset selection, baseline comparisons, and both model-based and human evaluations.  \n\u2003\u2013 Clear guidance on configuration management and dependency handling to facilitate a smooth transition to final code implementation.  \n\nBy following these actionable steps and addressing noted ambiguities, we can create a reproducible, efficient system that faithfully implements and validates the experiments described in the original paper.\n\n-----\n\n## Design\n[CONTENT]\n{\n    \"Implementation approach\": \"We will build a modular, pipeline-based system that reproduces the paper's method by decomposing the process into four main stages: paper parsing, planning, analysis, and code generation. The system uses open-source libraries such as Hugging Face Transformers for LLM interfacing, the OpenReview API for collecting paper metadata, and s2orc-doc2json for PDF-to-JSON conversion. Each stage is abstracted into its own module ensuring clear separation of concerns and ease of maintenance. The main entry point (main.py) orchestrates these modules to generate a complete, executable code repository that faithfully implements the reproduction of the paper's experiments.\",\n    \"File list\": [\n        \"main.py\",\n        \"paper_parser.py\",\n        \"planner.py\",\n        \"analyzer.py\",\n        \"code_generator.py\",\n        \"evaluation.py\"\n    ],\n    \"Data structures and interfaces\": \"classDiagram\\n    class Main {\\n        +__init__(config: dict)\\n        +run_pipeline()\\n    }\\n    class PaperParser {\\n        +__init__(paper_json: dict)\\n        +parse() -> Paper\\n    }\\n    class Planner {\\n        +__init__(paper: Paper)\\n        +create_overall_plan() -> dict\\n        +generate_architecture_design() -> dict\\n        +generate_config() -> dict\\n    }\\n    class Analyzer {\\n        +__init__(plan: dict)\\n        +analyze_modules() -> list\\n    }\\n    class CodeGenerator {\\n        +__init__(analysis: list, plan: dict)\\n        +generate_code() -> dict\\n    }\\n    class Evaluator {\\n        +__init__(repository: dict)\\n        +evaluate() -> dict\\n    }\\n    Main --> PaperParser\\n    Main --> Planner\\n    Main --> Analyzer\\n    Main --> CodeGenerator\\n    Main --> Evaluator\",\n    \"Program call flow\": \"sequenceDiagram\\n    participant M as Main\\n    participant PP as PaperParser\\n    participant PL as Planner\\n    participant AN as Analyzer\\n    participant CG as CodeGenerator\\n    participant EV as Evaluator\\n    M->>PP: Load paper JSON and call parse()\\n    PP-->>M: Return Paper object\\n    M->>PL: Pass Paper; call create_overall_plan()\\n    PL-->>M: Return overall plan, architecture design, and config\\n    M->>AN: Send plan for detailed module analysis\\n    AN-->>M: Return file-level analysis\\n    M->>CG: Trigger code generation with plan and analysis\\n    CG-->>M: Return generated repository files\\n    M->>EV: Evaluate repository reproducibility\\n    EV-->>M: Return evaluation metrics\",\n    \"Anything UNCLEAR\": \"Need clarification on the exact JSON schema for the input paper, explicit hyperparameters for model training, and integration details with external APIs (e.g., OpenReview and s2orc-doc2json) for automated paper data collection.\"\n}\n[/CONTENT]\n\n-----\n\n## Task\n[CONTENT]\n{\n    \"Required packages\": [\n        \"numpy==1.21.0\",\n        \"torch==1.9.0\",\n        \"transformers>=4.0.0\"\n    ],\n    \"Required Other language third-party packages\": [\n        \"No third-party dependencies required\"\n    ],\n    \"Logic Analysis\": [\n        [\n            \"paper_parser.py\",\n            \"PaperParser class: Parses the input paper JSON (and optionally uses s2orc-doc2json if needed) to extract structured information (abstract, sections, figures). Imports standard JSON libraries and helper utilities for text processing.\"\n        ],\n        [\n            \"planner.py\",\n            \"Planner class: Takes the structured Paper object and generates an overall plan, architecture design (including file lists and UML representations), and configuration details. Depends on the paper content produced by PaperParser and composes outputs to feed into later modules.\"\n        ],\n        [\n            \"analyzer.py\",\n            \"Analyzer class: Analyzes the plan generated by Planner to produce detailed file-level specifications. It identifies dependencies between modules, defines input/output contracts for each file, and ensures that the execution order is maintained. Relies on outputs from Planner.\"\n        ],\n        [\n            \"code_generator.py\",\n            \"CodeGenerator class: Uses the analysis from Analyzer along with the overall plan to generate modular and dependency-aware code for each component. Implements functions to generate code templates and inject configuration values. Must follow strict dependency ordering provided by Analyzer.\"\n        ],\n        [\n            \"evaluation.py\",\n            \"Evaluator class: Provides methods to conduct both reference-based and reference-free evaluations of the generated repository. Leverages external LLM-based evaluation models (e.g., GPT-4o) and computes correctness metrics. Integrates with configuration from config.yaml for hyperparameters.\"\n        ],\n        [\n            \"main.py\",\n            \"Main entry point: Orchestrates the entire pipeline by initializing configuration (from config.yaml), invoking PaperParser to obtain the structured paper, then sequentially calling Planner, Analyzer, CodeGenerator, and finally Evaluator. Ensures end-to-end flow and error handling across modules.\"\n        ]\n    ],\n    \"Task list\": [\n        \"paper_parser.py\",\n        \"planner.py\",\n        \"analyzer.py\",\n        \"code_generator.py\",\n        \"evaluation.py\",\n        \"main.py\"\n    ],\n    \"Full API spec\": \"openapi: 3.0.0\\ninfo:\\n  title: PaperCoder Reproduction API\\n  version: 1.0.0\\npaths:\\n  /evaluate:\\n    post:\\n      summary: Evaluate generated repository\\n      requestBody:\\n        required: true\\n        content:\\n          application/json:\\n            schema:\\n              type: object\\n              properties:\\n                repository:\\n                  type: object\\n      responses:\\n        '200':\\n          description: Evaluation metrics response\\n          content:\\n            application/json:\\n              schema:\\n                type: object\\n                properties:\\n                  metrics:\\n                    type: object\\n\",\n    \"Shared Knowledge\": \"All modules share a common configuration loaded from config.yaml, utility functions for logging and error handling, and consistent data structures for representing the paper and analysis results. Reusable helper functions for JSON parsing, dependency checks, and template-based code generation are assumed available.\",\n    \"Anything UNCLEAR\": \"Clarification needed on the exact JSON schema for the input paper, specific expected hyperparameters from config.yaml, and the detailed evaluation criteria (including model parameters for reference-based/reference-free scoring).\"\n}\n[/CONTENT]\n\n-----\n\n## Configuration file\n```yaml\n## config.yaml\ntraining:\n  learning_rate: \"...\"  ## Learning rate not specified in paper; set as needed.\n  batch_size: \"...\"     ## Batch size not specified in paper; set as needed.\n  epochs: \"...\"         ## Number of epochs not specified in paper; set as needed.\nevaluation:\n  n_way_sampling: 8\n  evaluation_model: \"o3-mini-high\"\n```\n-----\n\n## Instruction\nConduct a Logic Analysis to assist in writing the code, based on the paper, the plan, the design, the task and the previously specified configuration file (config.yaml). \nYou DON'T need to provide the actual code yet; focus on a thorough, clear analysis.\n\nWrite the logic analysis in 'main.py', which is intended for 'Main entry point: Orchestrates the entire pipeline by initializing configuration (from config.yaml), invoking PaperParser to obtain the structured paper, then sequentially calling Planner, Analyzer, CodeGenerator, and finally Evaluator. Ensures end-to-end flow and error handling across modules.'.\n\n-----\n\n## Logic Analysis: main.py"}, {"role": "assistant", "content": "[CONTENT]\nLogic Analysis for main.py:\n\n1. Configuration Initialization:\n   \u2022 Load the configuration from \u201cconfig.yaml\u201d using a YAML parser.\n   \u2022 Extract and validate required settings \u2013 under the \u201ctraining\u201d section (learning_rate, batch_size, epochs as placeholders) and the \u201cevaluation\u201d section (n_way_sampling, evaluation_model).\n   \u2022 Log or print a message confirming that the configuration has been successfully loaded.\n\n2. Input Paper Handling:\n   \u2022 Accept the paper JSON input (either via a file path or direct input).\n   \u2022 Instantiate the PaperParser with the paper JSON.\n   \u2022 Call the parse() method on the PaperParser instance to convert the raw paper JSON into a structured Paper object.\n   \u2022 Check for any parsing errors and handle exceptions accordingly.\n\n3. Planning Stage Execution:\n   \u2022 With the structured Paper object, instantiate the Planner.\n   \u2022 Invoke Planner.create_overall_plan() to generate a high-level overall plan from the paper.\n   \u2022 Optionally, call Planner.generate_architecture_design() to obtain the detailed file list, UML diagrams (class diagram), and sequence diagram illustrating module dependencies.\n   \u2022 Generate or retrieve configuration details via Planner.generate_config() if needed.\n   \u2022 Combine these outputs into a comprehensive \u201cplan\u201d dictionary that will be passed to subsequent modules.\n\n4. Analysis Stage Execution:\n   \u2022 Instantiate the Analyzer by supplying the \u201cplan\u201d dictionary obtained from the planning stage.\n   \u2022 Call Analyzer.analyze_modules() to produce detailed file-level specifications.\n   \u2022 The returned analysis should include dependency ordering and specify input/output contracts for each module.\n   \u2022 Validate that the analysis covers all files listed in the plan.\n\n5. Code Generation Stage Execution:\n   \u2022 Instantiate the CodeGenerator with two inputs: the plan dictionary (which includes overall plan and architecture) and the analysis list from the Analyzer.\n   \u2022 Call CodeGenerator.generate_code() to create a dictionary of repository files (mapping file names to generated code content).\n   \u2022 Ensure that the code generation follows the dependency ordering provided by the Analyzer.\n   \u2022 Log the completion of this stage and check for any inconsistencies in the generated repository.\n\n6. Evaluation Stage Execution:\n   \u2022 Instantiate the Evaluator with the generated repository (the dictionary of files from the CodeGenerator).\n   \u2022 Call Evaluator.evaluate() to perform a model-based evaluation (using both reference-based and reference-free approaches) of the generated code.\n   \u2022 The Evaluator should internally use the evaluation parameters from the configuration (e.g., n_way_sampling and evaluation_model set to \u201co3-mini-high\u201d).\n   \u2022 Capture the evaluation metrics as a dictionary.\n\n7. Orchestration and Error Handling:\n   \u2022 Sequentially execute the stages in the following order: Paper Parsing \u2192 Planning \u2192 Analysis \u2192 Code Generation \u2192 Evaluation.\n   \u2022 Implement try/except blocks around each stage to catch and log errors, ensuring that any exception does not cause unhandled crashes.\n   \u2022 Log progress messages at each stage for debugging and tracing.\n   \u2022 After completing all stages, output the evaluation metrics to the console or save them in a report, indicating the overall reproducibility and correctness score.\n\n8. Summary of Data Flow and Dependencies:\n   \u2022 main.py acts as the central coordinator, importing and invoking:\n       \u2013 PaperParser (from paper_parser.py) to handle input paper conversion.\n       \u2013 Planner (from planner.py) to create the overall plan and design artifacts.\n       \u2013 Analyzer (from analyzer.py) to analyze and structure file-level specifications with dependency details.\n       \u2013 CodeGenerator (from code_generator.py) to generate modular code for each defined file.\n       \u2013 Evaluator (from evaluation.py) to assess the generated repository.\n   \u2022 All modules rely on shared configuration from config.yaml and use consistent data structures as specified in the \u201cData structures and interfaces\u201d design.\n   \u2022 The execution order (as also defined in the Program call flow diagram) is strictly maintained, ensuring that outputs from one stage serve as inputs for the following stage.\n\nBy following this organized control flow and using comprehensive error handling, main.py will effectively orchestrate the entire end-to-end reproduction pipeline, ensuring that the logic adheres strictly to the paper\u2019s methodology and the provided design specifications.\n[/CONTENT]"}]